# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive/')

# Install Keras
!pip install -q keras

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, classification_report
import os

# Define dataset directories
train_dir = '/content/drive/MyDrive/Datasets/Datasets/train'
validation_dir = '/content/drive/MyDrive/Datasets/Datasets/validate'

# Define parameters
image_size = (256, 256)
batch_size = 32

# Load training and validation datasets
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

train_set = train_datagen.flow_from_directory(
    train_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical'
)

validation_datagen = ImageDataGenerator(rescale=1./255)

validation_set = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(128, 128),
    batch_size=batch_size,
    class_mode='categorical'
)

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3)

# Define and compile the model
base_model = ResNet50(include_top=False, input_shape=(256, 256, 3))
x = Flatten()(base_model.output)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
x = BatchNormalization()(x)
predictions = Dense(len(train_set.class_indices), activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_set,
    steps_per_epoch=train_set.samples // batch_size,
    epochs=10,
    validation_data=validation_set,
    validation_steps=validation_set.samples // batch_size,
    callbacks=[early_stopping]
)

# Evaluate the model
loss, accuracy = model.evaluate(validation_set, verbose=1)
print(f'Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}')

# Generate classification report and confusion matrix
Y_pred = model.predict(validation_set)
y_pred = np.argmax(Y_pred, axis=1)
print('Classification Report:')
print(classification_report(validation_set.classes, y_pred, target_names=train_set.class_indices))

cm = confusion_matrix(validation_set.classes, y_pred)
print('Confusion Matrix:')
print(cm)
